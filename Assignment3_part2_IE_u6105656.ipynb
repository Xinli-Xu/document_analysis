{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Assignment 3 Part 2: IE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Overview\n",
    "\n",
    "In this assignment, the task is to code a Named Entity Recognizer (NER) application in Python using the CRFsuite library.\n",
    "\n",
    "It is recommended you complete the Named_Entity_Extraction_Tutorial.ipynb tutorial before attemping this.\n",
    "\n",
    "Your tasks for this assignment are to:\n",
    "1. Build a NER classifier following the tutorial.\n",
    "2. Improve the performance of your NER classifier.\n",
    "3. Answer three written assignments.\n",
    "\n",
    "* Write answers in this notebook file, and upload the file to Wattle submission site. **Please rename and submit jupyter notebook file (Assignment5.ipynb) to your_uid.ipynb (e.g. u6000001.ipynb) with your written answers therein**. Do not upload any other files to Wattle except this notebook file."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <span style=\"color:blue\"> Question 1 (2 points) Build a NER model <a id='Task1'></a> </span>\n",
    "### Part A (1.5 marks)\n",
    "\n",
    "* Build a NER model using the train and test data files.\n",
    "* You can use the code provided in [tutorial sheet](Named_Entity_Extraction_Tutorial.ipynb) \n",
    "* Try changing the feature extraction, model hyper parameters, or other settings in order to improve your model performance.\n",
    "* Marks will be awarded based on how well your model performs.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sklearn version: 0.21.3\n",
      "Libraries succesfully loaded!\n",
      "Train and Test data loaded succesfully!\n",
      "Feature Extraction done!\n"
     ]
    }
   ],
   "source": [
    "### YOUR CODE HERE\n",
    "from __future__ import print_function\n",
    "from sklearn.metrics import confusion_matrix\n",
    "import io\n",
    "import nltk\n",
    "import scipy\n",
    "import codecs\n",
    "import sklearn\n",
    "import pycrfsuite\n",
    "import pandas as pd\n",
    "from itertools import chain\n",
    "from sklearn.preprocessing import LabelBinarizer\n",
    "from sklearn.metrics import classification_report\n",
    "\n",
    "print('sklearn version:', sklearn.__version__)\n",
    "print('Libraries succesfully loaded!')\n",
    "def sent2features(sent, feature_func):\n",
    "    return [feature_func(sent, i) for i in range(len(sent))]\n",
    "\n",
    "def sent2labels(sent):\n",
    "    return [s[-1] for s in sent]\n",
    "\n",
    "def sent2tokens(sent):\n",
    "    return [s[0] for s in sent]\n",
    "\n",
    "def bio_classification_report(y_true, y_pred):\n",
    "    \"\"\"\n",
    "    Classification report for a list of BIO-encoded sequences.\n",
    "    It computes token-level metrics and discards \"O\" labels.\n",
    "    \n",
    "    Note that it requires scikit-learn 0.15+ (or a version from github master)\n",
    "    to calculate averages properly!\n",
    "    \"\"\"\n",
    "    lb = LabelBinarizer()\n",
    "    y_true_combined = lb.fit_transform(y_true)\n",
    "    y_pred_combined = lb.transform(y_pred)\n",
    "        \n",
    "    tagset = set(lb.classes_) - {'O'}\n",
    "    tagset = sorted(tagset, key=lambda tag: tag.split('-', 1)[::-1])\n",
    "    class_indices = {cls: idx for idx, cls in enumerate(lb.classes_)}\n",
    "    \n",
    "    return classification_report(\n",
    "        y_true_combined,\n",
    "        y_pred_combined,\n",
    "        labels = [class_indices[cls] for cls in tagset],\n",
    "        target_names = tagset,\n",
    "    )\n",
    "            \n",
    "def word2simple_features(sent, i):\n",
    "    '''\n",
    "    This makes a simple baseline.  \n",
    "    You can add and/or remove features to get (much?) better results.\n",
    "    Experiment with it as you will need to do this for assignment.\n",
    "    '''\n",
    "    word = sent[i][0]\n",
    "    \n",
    "    features = {\n",
    "        'bias': 1.0, \n",
    "        'word.lower()': word.lower(), \n",
    "        'word[-3:]': word[-3:],\n",
    "        'word[-2:]': word[-2:],\n",
    "        'word.isupper()': word.isupper(),\n",
    "        'word.istitle()': word.istitle(),\n",
    "        'word.isdigit()': word.isdigit(),\n",
    "    }\n",
    "    if i > 0:\n",
    "        word1 = sent[i-1][0]\n",
    "        features.update({\n",
    "            '-1:word.lower()': word1.lower(),\n",
    "            '-1:word.istitle()': word1.istitle(),\n",
    "            '-1:word.isupper()': word1.isupper(),\n",
    "        })\n",
    "    else:\n",
    "        features['BOS'] = True\n",
    "    if i < len(sent)-1:\n",
    "        word1 = sent[i+1][0]\n",
    "        features.update({\n",
    "            '+1:word.lower()': word1.lower(),\n",
    "            '+1:word.istitle()': word1.istitle(),\n",
    "            '+1:word.isupper()': word1.isupper(),\n",
    "        })\n",
    "    else:\n",
    "        features['EOS'] = True\n",
    "    return features\n",
    "\n",
    "# load data and preprocess\n",
    "def extract_data(path):\n",
    "    \"\"\"\n",
    "    Extracting data from train file or test file. \n",
    "    path - the path of the file to extract\n",
    "    \n",
    "    return:\n",
    "        res - a list of sentences, each sentence is a\n",
    "              a list of tuples. For train file, each tuple\n",
    "              contains token and label. For test file, each\n",
    "              tuple only contains token.\n",
    "        ids - a list of ids for the corresponding token. This\n",
    "              is mainly for Kaggle submission.\n",
    "    \"\"\"\n",
    "    file = io.open(path, mode=\"r\", encoding=\"utf-8\")\n",
    "    next(file)\n",
    "    res = []\n",
    "    ids = []\n",
    "    sent = []\n",
    "    for line in file:\n",
    "        if line != '\\n':\n",
    "            # Each line contains the position ID, the token, and (for the training set) the label.\n",
    "            parts = line.strip().split(' ')\n",
    "            sent.append(tuple(parts[1:]))\n",
    "            ids.append(parts[0])\n",
    "        else:\n",
    "            res.append(sent)\n",
    "            sent = []\n",
    "                \n",
    "    return res, ids\n",
    "# Load train and test data\n",
    "train_data, train_ids = extract_data('train')\n",
    "test_data, test_ids = extract_data('test')\n",
    "\n",
    "# Load true labels for test data\n",
    "test_labels = list(pd.read_csv('test_ground_truth').loc[:, 'label'])\n",
    "\n",
    "print('Train and Test data loaded succesfully!')\n",
    "\n",
    "# Feature extraction using the word2simple_features function\n",
    "train_features = [sent2features(s, feature_func=word2simple_features) for s in train_data]\n",
    "train_labels = [sent2labels(s) for s in train_data]\n",
    "test_features = [sent2features(s, feature_func=word2simple_features) for s in test_data]\n",
    "\n",
    "trainer = pycrfsuite.Trainer(verbose=False)\n",
    "for xseq, yseq in zip(train_features, train_labels):\n",
    "    trainer.append(xseq, yseq)\n",
    "print('Feature Extraction done!')    \n",
    "\n",
    "# Explore the extracted features    \n",
    "sent2features(train_data[0], word2simple_features)\n",
    "trainer.params()\n",
    "trainer.set_params({\n",
    "    'c1': 0.01,   # coefficient for L1 penalty\n",
    "    'c2': 1e-2,  # coefficient for L2 penalty\n",
    "    'max_iterations': 100,  # stop earlier\n",
    "\n",
    "    # include transitions that are possible, but not observed\n",
    "    'feature.possible_transitions': True\n",
    "})\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training done :)\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "       B-LOC       0.83      0.82      0.82      2067\n",
      "       I-LOC       0.75      0.75      0.75       759\n",
      "      B-MISC       0.61      0.74      0.67       715\n",
      "      I-MISC       0.60      0.61      0.61      1228\n",
      "       B-ORG       0.84      0.87      0.86      3090\n",
      "       I-ORG       0.82      0.81      0.81      2237\n",
      "       B-PER       0.90      0.92      0.91      1850\n",
      "       I-PER       0.94      0.94      0.94      1634\n",
      "\n",
      "   micro avg       0.81      0.83      0.82     13580\n",
      "   macro avg       0.79      0.81      0.80     13580\n",
      "weighted avg       0.82      0.83      0.82     13580\n",
      " samples avg       0.10      0.10      0.10     13580\n",
      "\n",
      "CPU times: user 20.5 s, sys: 163 ms, total: 20.7 s\n",
      "Wall time: 20 s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "//anaconda3/lib/python3.7/site-packages/sklearn/metrics/classification.py:1437: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in samples with no predicted labels.\n",
      "  'precision', 'predicted', average, warn_for)\n",
      "//anaconda3/lib/python3.7/site-packages/sklearn/metrics/classification.py:1439: UndefinedMetricWarning: Recall and F-score are ill-defined and being set to 0.0 in samples with no true labels.\n",
      "  'recall', 'true', average, warn_for)\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "trainer.train('ner-esp.model')\n",
    "\n",
    "print('Training done :)')\n",
    "# Make predictions\n",
    "tagger = pycrfsuite.Tagger()\n",
    "tagger.open('ner-esp.model')\n",
    "test_pred = [tagger.tag(xseq) for xseq in test_features]\n",
    "test_pred = [s for w in test_pred for s in w]\n",
    "\n",
    "## Print evaluation\n",
    "print(bio_classification_report(test_pred, test_labels))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The output of the above cell should look something like this (but with different numbers)\n",
    "\n",
    "                precision    recall  f1-score   support\n",
    "\n",
    "      B-LOC       0.68      0.47      0.55      1084\n",
    "      I-LOC       0.52      0.25      0.34       325\n",
    "     B-MISC       0.54      0.11      0.19       339\n",
    "     I-MISC       0.54      0.22      0.32       557\n",
    "      B-ORG       0.76      0.51      0.61      1400\n",
    "      I-ORG       0.67      0.44      0.53      1104\n",
    "      B-PER       0.73      0.68      0.71       735\n",
    "      I-PER       0.78      0.82      0.80       634\n",
    "\n",
    "avg / total       0.68      0.48      0.55      6178\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Part B (0.5 marks)\n",
    "\n",
    "Briefly explain what changes to your model you tried and how these changes affected the model's performance."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- I change the implementation for 'word2simple_features' function which considers defines more features based on word identify, suffix and shape.\n",
    "\n",
    "- I change the coefficient for L1 penalty from 100 to 0.01. Since the L1 adds the 'absolute value of magnitude' of coefficient as penalty term to the loss function. If the coefficient is set to a large value, the model will be under-fit (as output shown: precision is low). Test with 0,0.001,0.01,0.1,0.5. 0.01 gives the best performance.\n",
    "\n",
    "- I change the coefficient for Le penalty from 'le-3' to 'le-2' which aims to avoid over-fitting. Test with 'le-2','le-3','le-4'. le-2 gives the best performance.\n",
    "\n",
    "- I change the 'max_iterations' from 50 to 100. The iterations affects how well the model is trained. For larger iterations, the model is trained better for the training data. However, if the iteration is set to a extremely large value, the model could over-fit the training data. So, I only increase the iteration 50 rounds. 100 and 300 have the similiar performance.\n",
    "\n",
    "- After apply the above changes, the model's overall performance has huge increase. The 'support' also have huge increases."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <span style=\"color:blue\"> Written Part (3 points) </span>\n",
    "\n",
    "Answer briefly and concisely the following questions.\n",
    "Check [this](https://sourceforge.net/p/jupiter/wiki/markdown_syntax/#md_ex_lists) if you are not familiar with markdown syntax."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question 2 (0.5 point)\n",
    "Think of three relevant baselines for the Named Entity Classification task.\n",
    "Provide answers using bullet list with 3 items. Give a short description of each of them."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Random Assignment: ramdomly assign a label to an entity. By testing the random assignment performance and your model's performance.\n",
    "- Simple heuristic: define simple rule for quick building. For example, set a rule that the word with a captical character is a person name. By testing the entity with simple heuristic and your classficiation to test your model's performance.\n",
    "- Simple machine learning techniques: using a defined machine learning skill like Navie Bayes. By testing the navie bayes' performance and your model's performance."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question 3 (1.5 point)\n",
    "How does Maximal Marginal Relevance (MMR) address redundancy issues? (0.5 point)\n",
    "\n",
    "How can you tell MMR that \"Sydney\" and \"Melbourne\" are cities? (0.5 points)\n",
    "\n",
    "How can you tell MMR that \"solar panels\" and \"photovoltaic cells\" have similar meaning? (0.5 points)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- The MMR address the redundancy issues by using Sim2 to measure the similarity between the two sentences. In here, it measures the similarity between the current sentence and the sentences already selected to be summary. For example, the current sentence s1 is similar to a sentence s2 in selected set R. The value of $Sim_{1}$(s1,s2) is large. By minusing (1-$\\lambda$)$Sim_{1}$(s1,s2), the score for sentence s1 is very low. So the sentence s1 will not be selected to be a part of summary which avoids the redundancy.\n",
    "- To tell MMR 'Sydney' and 'Melbourne' are cities, we could set Q to 'city'. By setting the $\\lambda$ to 1, we could get a relevance between 'City' and D. When D is 'Sydney', the relevance between 'City' and 'Sydney' is calculated. When D is 'Melbourne', the relevance between 'City' and 'Melbourne' is calculated. Since these two values are high (Sydney and Melbourne has label 'City'), the MMR knows the 'Sydney' and 'Melbourne' are cities.\n",
    "- To tell MMR that 'solar panels' and 'photovolatic cells' have similar meaning, we could directly calculate the similarity between these two. We could set $\\lambda$ to 1, D to 'solar panels' and Q to 'photovolatic cells'."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question 4 (1 point)\n",
    "\n",
    "Imagine you are developing an extractive text summarization tool using HMM.\n",
    "\n",
    "What are the hidden states and the observations of the HMM model? (0.5 point)\n",
    "\n",
    "Which algorithm is used to compute the probability of a particular observation sequence? (0.5 point)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- In this case, the hidden states are 'summary state' and 'non summary state'. The observations are 3 features: Position of sentence in the document, number of terms in the sentence and likelihood of the sentences given the document terms.\n",
    "- Forward algorithm: $$\\alpha_{t}(j)=\\sum_{i=1}^N\\alpha_{t-1}(i)\\alpha_{ij}b{j}b_{j}(O_{t})$$\n",
    "where $\\alpha_{t-1}$ is the previous path probability, $\\alpha_{ij}$ is the transition probability from the previous state to the current state, $b_{j}(O_{t})$ is the state observation likelihood of the oberservation $O_{t}$ given the current state j\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
