{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Assignment 2: Part 1 (ML) [7 pts]\n",
    "\n",
    "<span style=\"color:blue\">\n",
    "    \n",
    "### In sections 1.2-1.5 of the Machine Learning notebook, there are tasks for you to complete. Be sure to submit BOTH the Machine Learning demo notebook and this notebook.\n",
    "</spaN>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Assignment 2: Part 2 (NLP) [8 pts]\n",
    "\n",
    "### 2.1 Fast Text [3 pts]\n",
    "\n",
    "FastText[1] is a neural network based text classification model designed to be computationally efficient. Your task is to implement the FastText algorithm by completeing the code in the following cells. You will need to read through the provided fastText.pdf paper, which explains the algorithm. You do not need to implement Hierarchical softmax (2.1) or N-gram features (2.2), you only need to implement the basic architecture described in (2). \n",
    "\n",
    "The FastText model will be trained using mini-batch gradient descent. When the training data are sequences of variable lengths we can not simply stack multiple training sequences into one tensor. Instead, it is common to assume that there is a maximal sequence length, so that all sequences in a batch are fitted into tensors of the same dimensions. For sequences shorter than the maximal length, we append them with a special pad word so that all sequences in a batch are of the same length. A *pad word* is a special token, whose embedding is an all-zero vector, so that the presence of pad words does not change the output of the model. In this code, the pad word has an ID of 0, when implementing your embeddings you should ensure that this ID is always embedded to a vector of all zeros. Additionally, you will need to know how many words are in each input sentence (before they got padded to be the same length), this is provided as an input parameter to your FastText model.\n",
    "\n",
    "[1] Joulin, Armand and Grave, Edouard and Bojanowski, Piotr and Mikolov, Tomas. Bag of Tricks for Efficient Text Classification. arXiv preprint arXiv:1607.01759., 2016. [INCLUDED AS PART OF ASSIGNMENT 2 .ZIP PACKAGE]\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to /Users/blimmmmk/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "# coding: utf-8\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import numpy as np\n",
    "\n",
    "import collections\n",
    "import math\n",
    "import os\n",
    "import random\n",
    "\n",
    "import nltk\n",
    "nltk.download('punkt')\n",
    "from nltk import word_tokenize\n",
    "from collections import namedtuple\n",
    "\n",
    "import sys, getopt\n",
    "\n",
    "from random import shuffle\n",
    "\n",
    "\n",
    "num_classes = 3\n",
    "\n",
    "learning_rate = 0.005\n",
    "num_epochs = 3\n",
    "batch_size = 10\n",
    "embedding_dim = 10"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<span style=\"color:blue\">\n",
    "    \n",
    "### You need to complete the foward() and __init__() functions below [3 pts]\n",
    "</span>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a linear regression model\n",
    "class LinearRegressor(torch.nn.Module):\n",
    "    def __init__(self, d_in, d_out):\n",
    "        super(LinearRegressor, self).__init__()\n",
    "        self.W = torch.nn.Linear(d_in, d_out) \n",
    "    def lin_forward(self, x):\n",
    "        y_h = self.W(x)\n",
    "        return y_h\n",
    "\n",
    "# mini-batch explaination: https://machinelearningmastery.com/gentle-introduction-mini-batch-gradient-descent-configure-batch-size/\n",
    "class FastText(nn.Module):\n",
    "    \"\"\"Define the computation graph for fasttext model.\"\"\"\n",
    "    \n",
    "    def __init__(self, vocab_size, num_classes, embedding_dim, learning_rate):\n",
    "        \"\"\"Init the model with default parameters/hyperparameters.\"\"\"\n",
    "        super(FastText, self).__init__()\n",
    "        self.num_classes = num_classes\n",
    "        self.embedding_dim = embedding_dim\n",
    "        self.learning_rate = learning_rate\n",
    "        self.loss_func = F.cross_entropy\n",
    "        # TODO: create all the variables (weights) that the model needs here\n",
    "        self.vocab_size = vocab_size\n",
    "        self.embedder = nn.Embedding(vocab_size,embedding_dim)\n",
    "        self.lnmodel = LinearRegressor(embedding_dim,num_classes)\n",
    "        self.optimizer = torch.optim.SGD(self.parameters(), lr=learning_rate)                \n",
    "    \n",
    "    def forward(self, x, sens_lengths):\n",
    "        # embedding the sentence\n",
    "        embedding = self.embedder(x)\n",
    "        sum_embedding = torch.sum(embedding,dim=1)\n",
    "        # calculate the average embedding for the sentence\n",
    "        average_embedding = torch.div(sum_embedding,sens_lengths)\n",
    "        # use the linear regression model to predict the probability for each class\n",
    "        results = self.lnmodel.lin_forward(average_embedding)\n",
    "        return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to /Users/blimmmmk/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "number of sequences is 8216. \n",
      "PAD word id is 0 .\n",
      "Unknown word id is 1 .\n",
      "size of vocabulary is 3666. \n",
      "read 1000 sentences from question_2-1_data/sentences_train.txt .\n",
      "read 500 sentences from question_2-1_data/sentences_dev.txt .\n",
      "read 500 sentences from question_2-1_data/sentences_test.txt .\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "//anaconda3/lib/python3.7/site-packages/torch/serialization.py:256: UserWarning: Couldn't retrieve source code for container of type FastText. It won't be checked for correctness upon loading.\n",
      "  \"type \" + obj.__name__ + \". It won't be checked \"\n",
      "//anaconda3/lib/python3.7/site-packages/torch/serialization.py:256: UserWarning: Couldn't retrieve source code for container of type LinearRegressor. It won't be checked for correctness upon loading.\n",
      "  \"type \" + obj.__name__ + \". It won't be checked \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0 : train loss = 1.8986031728982926 , validation accuracy = 0.3140000104904175 .\n",
      "Epoch 1 : train loss = 1.1226851260662079 , validation accuracy = 0.34200000762939453 .\n",
      "Epoch 2 : train loss = 1.1221101188659668 , validation accuracy = 0.3619999885559082 .\n",
      "Accuracy on the test set : 0.39399999380111694.\n"
     ]
    }
   ],
   "source": [
    "from fasttext import load_question_2_1, train_fast_text\n",
    "word_to_id, train_data, valid_data, test_data = load_question_2_1('question_2-1_data')\n",
    "model = FastText(len(word_to_id)+2, num_classes, embedding_dim=10, learning_rate=0.001)\n",
    "model_file_path = os.path.join('models', 'fasttext_model_file_q2-1')\n",
    "train_fast_text(model, train_data, valid_data, test_data, model_file_path, batch_size=batch_size, num_epochs=num_epochs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2 Question Classification [3 pts]\n",
    "\n",
    "Understanding questions is a key problem in chatbots and question answering systems. In the open-domain setting, it is difficult to find right answers in the huge search space. To tackle the problem, one approach is to categorize questions into a finite set of semantic classes, and each semantic class corresponds to a small answer space.\n",
    "\n",
    "<span style=\"color:blue\">\n",
    "    \n",
    "### Your task is to implement a question classification model in Pytorch, and apply it to the question_2_2_data provided in this assignment.\n",
    "</span>\n",
    "\n",
    "Notes: \n",
    "\n",
    "\n",
    "-  Please do NOT submit your data directories, pretrained word embeddings, and Pytorch library!\n",
    "\n",
    "-  You may consider reusing parts of the code above\n",
    "\n",
    "-  Code must be submitted with the assignment for purposes of plagiarism detection"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dataset\n",
    "\n",
    "The dataset provided contains three files: **train.json**, **validation.json**, and **test.json**, which are the training dataset, validation dataset, and the test dataset, respectively. \n",
    "See an example below: \n",
    "```\n",
    "{\n",
    "   \"ID\": S1,\n",
    "   \"Label\": 3,\n",
    "   \"Sentence\":\"What country has the best defensive position in the board game Diplomacy ?\"\n",
    "}\n",
    "```\n",
    "In the training set and the validation set, the response variable is called `Label`. Your task is to predict the `Label` for each sentence in the test set. \n",
    "\n",
    "### Evaluation\n",
    "\n",
    "The performance of your prediction will be evaluated automatically on Kaggle using __Accuracy__ , which is defined as the number of correct predictions divided by the total number of sentences in the test set (https://classeval.wordpress.com/introduction/basic-evaluation-measures/).\n",
    "\n",
    "It is important to understand that the leaderboard score will be only computed based on the half of the test cases, and the remaining half will be computed after the deadline based on your selected submission. This process will ensure that your performance is not only applicable for the known test cases, but also generalised to the unknown test cases. We will combine these two performances to score the first assignment.\n",
    "\n",
    "Your score will be computed using a lower bound and an upper bound, which will be shown on the Kaggle leader board. \n",
    "Achieving an accuracy equal and below the lower bound amounts to a grade of zero, while achieving the upper bound amounts to the full points (here 3 points, see score distribution here below).\n",
    "Consequently, your score for this competition task will be calculated based on:\n",
    "\n",
    "$$\n",
    "    \\operatorname{Your\\_Score} = \\frac{Your\\_Accuracy - Lower\\_Bound}{Upper\\_Bound - Lower\\_Bound} * 3\n",
    "$$\n",
    "Notes about the lower bound and upper bounds predictors:\n",
    "\n",
    "* The **lower bound** is the performance obtained by a classifer that always picks the majority class according to the class distribution in the training set.\n",
    "* The **upper bound** is generated by an \"in-house\" classifier trained on the same dataset that you were given.\n",
    "\n",
    "There are many possibilities to achieve better results than this. However, the **only** labeled training dataset to train your model should be the provided **train.json**. \n",
    "If you obtain a better performance than the upper bound, then you will have a grade higher than 3 points for this question. This can be useful to compensate for any lost points for the whole assignment.\n",
    "However, the total mark of this assignment is capped at 10 marks.\n",
    "\n",
    "### Kaggle competition\n",
    "\n",
    "- You will be given a link to join the competition during your labs.\n",
    "- Before submitting the result, first go to **team** menu and change your **team name** as **your university id**.\n",
    "- You need to upload the generated result file to Kaggle. The result file should be in the following format\n",
    "```\n",
    "id,category\n",
    "S101,0\n",
    "S201,1\n",
    "S102,2\n",
    "...\n",
    "```\n",
    "- Note that you are only allowed to upload **5 copies** of your results to Kaggle per day. Make every upload count, and don't waste your opportunities!\n",
    "\n",
    "**NB** you need to fill in the cells below with your code. If you fail to provide the code, you will get zero for this question. Your code should be well documented and provide methods to generate the prediction files and compute accuracy on the validation set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json # You can use this library to read the .json files into a Python dict: https://docs.python.org/2/library/json.html\n",
    "from nltk import word_tokenize # You can use this to tokenize strings, or do your own pre-processing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /Users/blimmmmk/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "number of sequences is 76218. \n",
      "PAD word id is 0 .\n",
      "Unknown word id is 1 .\n",
      "size of vocabulary is 8908. \n",
      "Epoch 0 : train loss = 1.0442944303936055 , validation accuracy = 0.8479999899864197 .\n",
      "Epoch 1 : train loss = 0.19806610321862653 , validation accuracy = 0.9300000071525574 .\n",
      "Epoch 2 : train loss = 0.07306939442177311 , validation accuracy = 0.9430000185966492 .\n",
      "Epoch 3 : train loss = 0.03766401666578003 , validation accuracy = 0.9020000100135803 .\n",
      "Epoch 4 : train loss = 0.017630691041499485 , validation accuracy = 0.9419999718666077 .\n",
      "Epoch 5 : train loss = 0.007170517406503648 , validation accuracy = 0.9490000009536743 .\n",
      "Epoch 6 : train loss = 0.0010669060805544323 , validation accuracy = 0.9490000009536743 .\n",
      "Epoch 7 : train loss = 0.0005552506205980711 , validation accuracy = 0.9480000138282776 .\n",
      "Epoch 8 : train loss = 0.0004076377811271971 , validation accuracy = 0.9470000267028809 .\n",
      "Epoch 9 : train loss = 0.0003155591940603406 , validation accuracy = 0.9459999799728394 .\n",
      "Epoch 10 : train loss = 0.00024015776922543672 , validation accuracy = 0.9459999799728394 .\n",
      "Epoch 11 : train loss = 0.0001853439060933822 , validation accuracy = 0.9470000267028809 .\n",
      "Epoch 12 : train loss = 0.00014435418180372864 , validation accuracy = 0.9459999799728394 .\n",
      "Epoch 13 : train loss = 0.00010932817346084419 , validation accuracy = 0.9480000138282776 .\n",
      "Epoch 14 : train loss = 8.519289999841726e-05 , validation accuracy = 0.9470000267028809 .\n",
      "Epoch 15 : train loss = 6.544481120378746e-05 , validation accuracy = 0.9459999799728394 .\n",
      "Epoch 16 : train loss = 5.40595234403257e-05 , validation accuracy = 0.9459999799728394 .\n",
      "Epoch 17 : train loss = 3.974572354448561e-05 , validation accuracy = 0.9449999928474426 .\n",
      "Epoch 18 : train loss = 2.9424739596282185e-05 , validation accuracy = 0.9459999799728394 .\n",
      "Epoch 19 : train loss = 2.3280383147501255e-05 , validation accuracy = 0.9449999928474426 .\n",
      "Epoch 20 : train loss = 1.7747226035310398e-05 , validation accuracy = 0.9449999928474426 .\n",
      "Epoch 21 : train loss = 1.3842735529143437e-05 , validation accuracy = 0.9449999928474426 .\n",
      "Epoch 22 : train loss = 1.063493324716275e-05 , validation accuracy = 0.9449999928474426 .\n",
      "Epoch 23 : train loss = 8.28129678449012e-06 , validation accuracy = 0.9449999928474426 .\n",
      "Epoch 24 : train loss = 6.062356773888437e-06 , validation accuracy = 0.9449999928474426 .\n",
      "Epoch 25 : train loss = 4.741758613456861e-06 , validation accuracy = 0.9459999799728394 .\n",
      "Epoch 26 : train loss = 3.6152186488318385e-06 , validation accuracy = 0.9449999928474426 .\n",
      "Epoch 27 : train loss = 2.852230606508224e-06 , validation accuracy = 0.9459999799728394 .\n",
      "Epoch 28 : train loss = 2.1770517015079304e-06 , validation accuracy = 0.9430000185966492 .\n",
      "Epoch 29 : train loss = 1.6851964915993508e-06 , validation accuracy = 0.9459999799728394 .\n",
      "Epoch 30 : train loss = 1.291473649829074e-06 , validation accuracy = 0.9459999799728394 .\n",
      "Epoch 31 : train loss = 1.0201943717627348e-06 , validation accuracy = 0.9430000185966492 .\n",
      "Epoch 32 : train loss = 7.936514029054591e-07 , validation accuracy = 0.9459999799728394 .\n",
      "Epoch 33 : train loss = 6.57795041447337e-07 , validation accuracy = 0.9449999928474426 .\n",
      "Epoch 34 : train loss = 4.6272029501932687e-07 , validation accuracy = 0.9470000267028809 .\n",
      "Epoch 35 : train loss = 3.60030780521825e-07 , validation accuracy = 0.9459999799728394 .\n",
      "Epoch 36 : train loss = 2.7474189241212755e-07 , validation accuracy = 0.9459999799728394 .\n",
      "Epoch 37 : train loss = 2.2543993441571258e-07 , validation accuracy = 0.9459999799728394 .\n",
      "Epoch 38 : train loss = 1.7498438075944e-07 , validation accuracy = 0.9459999799728394 .\n",
      "Epoch 39 : train loss = 1.3850060628122434e-07 , validation accuracy = 0.9459999799728394 .\n",
      "Epoch 40 : train loss = 1.1105364534421072e-07 , validation accuracy = 0.9470000267028809 .\n",
      "Epoch 41 : train loss = 8.443988339728609e-08 , validation accuracy = 0.9470000267028809 .\n",
      "Epoch 42 : train loss = 6.554906958083021e-08 , validation accuracy = 0.9470000267028809 .\n",
      "Epoch 43 : train loss = 5.149711946836602e-08 , validation accuracy = 0.9480000138282776 .\n",
      "Epoch 44 : train loss = 4.036130905837518e-08 , validation accuracy = 0.9470000267028809 .\n",
      "Epoch 45 : train loss = 3.0971978247317984e-08 , validation accuracy = 0.9470000267028809 .\n",
      "Epoch 46 : train loss = 2.380980900254246e-08 , validation accuracy = 0.9480000138282776 .\n",
      "Epoch 47 : train loss = 1.797752769589113e-08 , validation accuracy = 0.9459999799728394 .\n",
      "Epoch 48 : train loss = 1.4100021795299302e-08 , validation accuracy = 0.9459999799728394 .\n",
      "Epoch 49 : train loss = 1.152035860614441e-08 , validation accuracy = 0.9470000267028809 .\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "Your tasks are to\n",
    "    1. Read in the .json files and create Dataset objects from them. The dataset constructor requires two parameters: a list of\n",
    "        sentences (where each sentence is a list of word ids) and a list of labels (or None is there are no labels).\n",
    "        You will need to apply appropriate preprocessing to the raw text to get in the appropriate form.\n",
    "    2. Run the train_fast_text() function on these Datasets and your model.\n",
    "    3. Convert the output file of predictions into the correct format for Kaggle. \n",
    "        Kaggle expects a csv with two columns, id and category. You need to have these two column headers as the first row.\n",
    "        Your csv should not include any whitespace.\n",
    "    4. Change the model hyper parameters, training settings, text preprocessing, or anything else you see fit\n",
    "        in order to improve your models performance.\n",
    "\"\"\"\n",
    "\n",
    "num_classes = 6\n",
    "\n",
    "from prepros import preprocessor\n",
    "from fasttext import Dataset, build_vocab, map_token_seq_to_word_id_seq\n",
    "\n",
    "# paths to the files\n",
    "file_path_train = 'question_2-2_data/train.json'\n",
    "file_path_validation = 'question_2-2_data/validation.json'\n",
    "file_path_test = 'question_2-2_data/test.json'\n",
    "\n",
    "with open(file_path_train) as file1:\n",
    "    train_objects = json.load(file1)\n",
    "with open(\"raw_text.txt\", \"w\") as file:\n",
    "    for o in train_objects:\n",
    "        file.write(o['Sentence'] + \"\\n\")\n",
    "# create a file to store the sentences\n",
    "raw_text_path = \"raw_text.txt\"\n",
    "# only use the sentence to build the dictionary, filter out the useless information\n",
    "vocabulary = build_vocab(raw_text_path)\n",
    "# open the validation and test file and read the file\n",
    "with open(file_path_validation) as file2:\n",
    "    validation_objects = json.load(file2)\n",
    "with open(file_path_test) as file3:\n",
    "    test_objects = json.load(file3)\n",
    "# initialize lists\n",
    "train_labels,validation_labels= list(),list()\n",
    "train_sentences,validation_sentences,test_sentences = list(),list(),list()\n",
    "test_ids = list()\n",
    "# store the label, sentence and ids to the list\n",
    "for x in train_objects:\n",
    "    token_seq = word_tokenize(x['Sentence'])\n",
    "    train_sentences.append(map_token_seq_to_word_id_seq(token_seq, vocabulary))\n",
    "    train_labels.append(x['Label'])\n",
    "for x in validation_objects:\n",
    "    token_seq = word_tokenize(x['Sentence'])\n",
    "    validation_sentences.append(map_token_seq_to_word_id_seq(token_seq, vocabulary))\n",
    "    validation_labels.append(x['Label'])\n",
    "for x in test_objects:\n",
    "    token_seq = word_tokenize(x['Sentence'])\n",
    "    test_sentences.append(map_token_seq_to_word_id_seq(token_seq, vocabulary))\n",
    "    test_ids.append(x['ID'])\n",
    "train_dataset = Dataset(train_sentences, train_labels)\n",
    "valid_dataset = Dataset(validation_sentences,validation_labels)\n",
    "# no label for test\n",
    "test_dataset = Dataset(test_sentences)\n",
    "# optimize the model\n",
    "model = FastText(len(vocabulary)+2,num_classes,embedding_dim=200,learning_rate=0.01)\n",
    "model.optimizer = torch.optim.Adam(model.parameters(), lr=0.01) \n",
    "\n",
    "model_file_path = os.path.join('models', 'fasttext_model_file_q2-2')\n",
    "train_fast_text(model, train_dataset, valid_dataset, test_dataset, model_file_path, batch_size=15, num_epochs=50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# write the outcome to a csv file for uploading the result to the kaggle\n",
    "import csv\n",
    "# read the results\n",
    "result_path = \"models/fasttext_model_file_q2-2predictions.csv\"\n",
    "labels = list()\n",
    "with open(result_path) as file:\n",
    "    reader = csv.reader(file)\n",
    "    for row in reader:\n",
    "        # each row is a list, row[0] is the string\n",
    "        labels.append(row[0])\n",
    "# write the result csv\n",
    "with open(\"result.csv\",\"w\") as file:\n",
    "    writer = csv.writer(file)\n",
    "    writer.writerow([\"id\",\"category\"])\n",
    "    if(len(test_ids) == len(labels)):\n",
    "        for x in range(len(test_ids)):\n",
    "            writer.writerow([test_ids[x],labels[x]])\n",
    "    else:\n",
    "        print(\"size error\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.3 Comparison between Absolute Discounting and Kneser Ney smoothing [2pts]\n",
    "\n",
    "Read the code below for interpolated absolute discounting and implement Kneser Ney smoothing in Python. It is sufficient to assume that the highest order of ngram is two and the discount is 0.75. Evaluate your program on the following ngram corpus and compute the distribution $p(x | \\text{Granny})$ for all possible unigrams in the given corpus. \n",
    "\n",
    "<span style=\"color:blue\">\n",
    "    \n",
    "### Explain what make the differences regarding the prediction results between interpolated absolute discounting and Kneser Ney smoothing.\n",
    "</span>\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "ngram_corpus = ['Sam eats apple',\n",
    "          \"Granny plays with Sam\",\n",
    "           \"Sam plays with Smith\",\n",
    "           \"Sam likes Smith\",\n",
    "          \"Sam likes apple\",\n",
    "                \"Sam likes sport\",\n",
    "                \"Sam plays tennis\",\n",
    "                \"Sam likes games\",\n",
    "                \"Sam plays games\",\n",
    "          \"Sam likes apple Granny Smith\"]\n",
    "\n",
    "from nltk import word_tokenize\n",
    "from nltk.util import ngrams\n",
    "from collections import Counter\n",
    "\n",
    "class NgramStats:\n",
    "    \"\"\" Collect unigram and bigram statistics. \"\"\"\n",
    "    \n",
    "    def __init__(self):\n",
    "        self.bigram_to_count = Counter([])\n",
    "        self.unigram_to_count = dict()\n",
    "        \n",
    "    def collect_ngram_counts(self, corpus):\n",
    "        \"\"\"Collect unigram and bigram counts from the given corpus.\"\"\"\n",
    "        unigram_counter = Counter([])\n",
    "        for sentence in corpus:\n",
    "            tokens = word_tokenize(sentence)\n",
    "            bigrams = ngrams(tokens, 2)\n",
    "            unigrams = ngrams(tokens, 1)\n",
    "            self.bigram_to_count += Counter(bigrams)\n",
    "            unigram_counter += Counter(unigrams)\n",
    "        self.unigram_to_count = {k[0]:int(v) for k,v in unigram_counter.items()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Counter({('Sam', 'likes'): 5, ('Sam', 'plays'): 3, ('plays', 'with'): 2, ('likes', 'apple'): 2, ('Sam', 'eats'): 1, ('eats', 'apple'): 1, ('Granny', 'plays'): 1, ('with', 'Sam'): 1, ('with', 'Smith'): 1, ('likes', 'Smith'): 1, ('likes', 'sport'): 1, ('plays', 'tennis'): 1, ('likes', 'games'): 1, ('plays', 'games'): 1, ('apple', 'Granny'): 1, ('Granny', 'Smith'): 1})\n",
      "{'Sam': 10, 'eats': 1, 'apple': 3, 'Granny': 2, 'plays': 4, 'with': 2, 'Smith': 3, 'likes': 5, 'sport': 1, 'tennis': 1, 'games': 2}\n"
     ]
    }
   ],
   "source": [
    "stats = NgramStats()         \n",
    "stats.collect_ngram_counts(ngram_corpus)\n",
    "print(stats.bigram_to_count)\n",
    "print(stats.unigram_to_count)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Interpolated Absolute Discounting\n",
    "import operator\n",
    "    \n",
    "class AbsDist:\n",
    "    \"\"\"\n",
    "     Implementation of Interpolated Absolute Discounting\n",
    "     \n",
    "     Reference: slide 25 in https://nlp.stanford.edu/~wcmac/papers/20050421-smoothing-tutorial.pdf\n",
    "    \"\"\"\n",
    "    def __init__(self, ngram_stats):\n",
    "        \"\"\" Initialization\n",
    "        \n",
    "            Args:\n",
    "                ngram_stats (NgramStats) : ngram statistics.\n",
    "        \"\"\"\n",
    "        self.unigram_freq = float(sum(ngram_stats.unigram_to_count.values()))\n",
    "        self.stats= ngram_stats\n",
    "    \n",
    "    def compute_prop(self, bigram, discount = 0.75):\n",
    "        \"\"\" Compute probability p(y | x)\n",
    "        \n",
    "            Args:\n",
    "                bigram (string tuple) : a bigram (x, y), where x and y denotes an unigram respectively.\n",
    "                discount (float) : the discounter factor for the linear interpolation.\n",
    "        \"\"\"\n",
    "        preceding_word_count = 0\n",
    "        if bigram[0] in self.stats.unigram_to_count:\n",
    "            preceding_word_count = self.stats.unigram_to_count[bigram[0]]\n",
    "            \n",
    "        if preceding_word_count > 0:\n",
    "            left_term = 0\n",
    "            if bigram in self.stats.bigram_to_count:\n",
    "                bigram_count = float(self.stats.bigram_to_count[bigram])\n",
    "                left_term = (bigram_count - discount)/preceding_word_count\n",
    "            right_term = 0\n",
    "            if bigram[1] in self.stats.unigram_to_count:\n",
    "                current_word_count = self.stats.unigram_to_count[bigram[1]]\n",
    "                num_bigram_preceding_word = 0\n",
    "                for c_bigram in self.stats.bigram_to_count.keys():\n",
    "                    if c_bigram[0] == bigram[0] :\n",
    "                        num_bigram_preceding_word += 1\n",
    "                normalization_param = (discount * num_bigram_preceding_word)/ preceding_word_count \n",
    "                p_unigram = current_word_count/self.unigram_freq\n",
    "                right_term = normalization_param * p_unigram\n",
    "            return left_term + right_term\n",
    "        \n",
    "        return 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('eats', 0.022058823529411763), ('sport', 0.022058823529411763), ('tennis', 0.022058823529411763), ('Granny', 0.044117647058823525), ('with', 0.044117647058823525), ('games', 0.044117647058823525), ('apple', 0.0661764705882353), ('likes', 0.11029411764705882), ('Smith', 0.19117647058823528), ('plays', 0.21323529411764705), ('Sam', 0.22058823529411764)]\n"
     ]
    }
   ],
   "source": [
    "def compute_prop_abs_dist(ngram_stats, preceding_unigram, d = 0.75):\n",
    "    \"\"\" Compute the distribution p(y | x) of all y given preceding_unigram\n",
    "\n",
    "        Args:\n",
    "            preceding_unigram (string) : the preceding unigram.\n",
    "            d (float) : the discounter factor for the linear interpolation.\n",
    "    \"\"\"\n",
    "    absDist = AbsDist(ngram_stats)\n",
    "    c_unigram_to_prob = dict()\n",
    "    for c_unigram in ngram_stats.unigram_to_count.keys():\n",
    "        if not c_unigram in c_unigram_to_prob:\n",
    "            c_unigram_to_prob[c_unigram] = absDist.compute_prop((preceding_unigram, c_unigram), d)\n",
    "  \n",
    "    sorted_prob = sorted(c_unigram_to_prob.items(), key=operator.itemgetter(1))\n",
    "    return sorted_prob\n",
    "\n",
    "print(compute_prop_abs_dist(stats, 'Granny'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('Sam', 0.046875), ('eats', 0.046875), ('Granny', 0.046875), ('with', 0.046875), ('likes', 0.046875), ('sport', 0.046875), ('tennis', 0.046875), ('apple', 0.09375), ('games', 0.09375), ('plays', 0.21875), ('Smith', 0.265625)]\n"
     ]
    }
   ],
   "source": [
    "class KNS:\n",
    "    \"\"\"\n",
    "     Implementation of Kneser-Ney Smoothing\n",
    "     \n",
    "    \"\"\"\n",
    "    def __init__(self, ngram_stats):\n",
    "        \"\"\" Initialization\n",
    "        \n",
    "            Args:\n",
    "                ngram_stats (NgramStats) : ngram statistics.\n",
    "        \"\"\"\n",
    "        self.unigram_freq = float(sum(ngram_stats.unigram_to_count.values()))\n",
    "        self.stats= ngram_stats\n",
    "    \n",
    "    def compute_prop(self, bigram, discount = 0.75):\n",
    "        \"\"\" Compute probability p(y | x)\n",
    "        \n",
    "            Args:\n",
    "                bigram (string tuple) : a bigram (x, y), where x and y denotes an unigram respectively.\n",
    "                discount (float) : the discounter factor for the linear interpolation.\n",
    "        \"\"\"\n",
    "        preceding_word_count = 0\n",
    "        if bigram[0] in self.stats.unigram_to_count:\n",
    "            preceding_word_count = self.stats.unigram_to_count[bigram[0]]\n",
    "            \n",
    "        if preceding_word_count > 0:\n",
    "            left_term = 0\n",
    "            if bigram in self.stats.bigram_to_count:\n",
    "                bigram_count = float(self.stats.bigram_to_count[bigram])\n",
    "                left_term = (bigram_count - discount)/preceding_word_count    \n",
    "            right_term = 0\n",
    "            num_bigram_preceding_word = 0\n",
    "            # store the number of bigrams that end with y\n",
    "            count = 0\n",
    "            for c_bigram in self.stats.bigram_to_count.keys():\n",
    "                # check whether the bigram starts with the x\n",
    "                if c_bigram[0] == bigram[0]:\n",
    "                    num_bigram_preceding_word += 1\n",
    "                if c_bigram[1] == bigram[1]:\n",
    "                    count += 1\n",
    "            right_term = (discount * num_bigram_preceding_word * count)/ (preceding_word_count * len(self.stats.bigram_to_count))\n",
    "            return left_term + right_term\n",
    "        return 0\n",
    "\n",
    "\n",
    "def compute_prop_KN(ngram_stats, preceding_word, d=0.75):\n",
    "    # Implement Kneser Ney Smoothing here.\n",
    "    # Hint: try to reuse the above code as much as possible.\n",
    "    # raise NotImplementedError\n",
    "    kns = KNS(ngram_stats)\n",
    "    dictionary = dict()\n",
    "    for c_unigram in ngram_stats.unigram_to_count.keys():\n",
    "        if not c_unigram in dictionary:\n",
    "            dictionary[c_unigram] = kns.compute_prop((preceding_word, c_unigram), d)\n",
    "    sorted_prob = sorted(dictionary.items(), key=operator.itemgetter(1))\n",
    "    return sorted_prob\n",
    "\n",
    "print(compute_prop_KN(stats, 'Granny'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<span style=\"color:blue\">\n",
    "    \n",
    "EXPLAIN THE DIFFERENCES REGARDING PREDICTION RESULTS HERE\n",
    "</span>\n",
    "\n",
    "Abosulte Discounting: $P(y|x)$ = $\\frac {max(count(x,y)-d,0)} {count(x)}$ + $\\lambda(x)$ $P(y)$ <br> <br>\n",
    "Kneser-Ney Smoothing: $P(y|x)$ = $\\frac {max(count(x,y)-d,0)} {count(x)}$ + $\\lambda(x)$ $P_{Continuation}$$(y)$\n",
    "\n",
    "The difference between two models is the calculation of probability y.\n",
    "\n",
    "\n",
    "The absolute-discounting interpolation model calculates the probability of the y. If y is more likely to use in context, the absolute-discounting interpolation probabilty is high. \n",
    "\n",
    "\n",
    "However, the Kneser-Ney Smoothing models considers the continuation probabilty of y which is propotional to the number of different words that y follows. If the y is more likely to follow different words, the Kneser-Ney Smoothing probability is high.\n",
    "\n",
    "From the ngram_corpus, we find the word 'Sam' appears most frequently which has the largest probabilty of P(y). So the absolute-discounting interpolation model shows the highest probaility for 'Sam'. The word 'Smith' appears most frequently as the end of a bigram ('Granny Smith','with Smith','like Smith'). So the Kenser-Ney Smoothing model shows the highest probabilty for 'Smith'."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Assignment is discussed with Simian Wang (u6165791)."
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
